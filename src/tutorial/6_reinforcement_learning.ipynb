{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "difficult-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "\n",
    "from lib.pysoarlib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-texas",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "This section will cover\n",
    "1. Reinforcement learning (RL) as a new kind of learning. \n",
    "RL will replace our use of preferences like *worst* or *better*.\n",
    "2. Combinatorial rules (using `gp`) as a way of creating a bunch of rules at once \n",
    "    (see `templates` in manual for other ways to do this)\n",
    "\n",
    "So far, when we've proposed operators, we tied *preferences* to them.\n",
    "These preferences would tell Soar which operator to select and apply when multiple operators were proposed.\n",
    "Instead of using preferences like *better* or *worst* we can use numbers to weight preferences.\n",
    "\n",
    "The agent will tweak these numbers itself through reinforcement learning.\n",
    "\n",
    "\"Reinforcement learning (RL) in Soar allows agents to alter behavior over time by dynamically changing numerical \n",
    "indifferent preferences in procedural memory in response to a reward signal. This learning mechanism contrasts \n",
    "starkly with chunking. Whereas chunking is a one-shot form of learning that increases agent execution performance \n",
    "by summarizing sub-goal results, RL is an incremental form of learning that probabilistically alters agent behavior.\"\n",
    "\n",
    "#### Left Right Agent\n",
    "\n",
    "Let's consider a simple agent that chooses to move left or right.\n",
    "Unbeknownst to the agent, \"right\" is always the correct choice.\n",
    "Once the agent makes a move, we will give it a reward.\n",
    "A +1 reward will be given if the agent moves right and -1 if the agent moves left.\n",
    "\n",
    "All this agent will do is pick a direction, receive a reward, and then halt.\n",
    "\n",
    "Let's look at the Soar code.\n",
    "This first cell is going to be code that is familiar to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "upset-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_initialization = \"\"\"\n",
    "sp {propose*initialize-left-right \n",
    "    (state <s> ^superstate nil\n",
    "              -^name)\n",
    "-->\n",
    "    (<s> ^operator <o> +)\n",
    "    (<o> ^name initialize-left-right)\n",
    "}\n",
    "\n",
    "sp {apply*initialize-left-right \n",
    "    (state <s> ^operator <op>)\n",
    "    (<op>      ^name     initialize-left-right)\n",
    "-->\n",
    "    (<s> ^name        left-right\n",
    "         ^direction    <d1> <d2>\n",
    "         ^location    start)\n",
    "    (<d1> ^name left  ^reward -1) \n",
    "    (<d2> ^name right ^reward  1)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "lr_operators = \"\"\"\n",
    "sp {left-right*propose*move \n",
    "    (state <s> ^name            left-right\n",
    "               ^direction.name  <dir> \n",
    "               ^location        start)\n",
    "-->\n",
    "    (<s> ^operator <op> +) \n",
    "    (<op> ^name move\n",
    "          ^dir <dir>)\n",
    "}\n",
    "\n",
    "sp {apply*move\n",
    "    (state <s> ^operator <op>\n",
    "               ^location start) \n",
    "    (<op> ^name move\n",
    "          ^dir <dir>)\n",
    "-->\n",
    "    (<s> ^location start - <dir>) \n",
    "    (write (crlf) |Moved: | <dir>)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "lr_halt_condition = \"\"\"\n",
    "sp {elaborate*done\n",
    "    (state <s> ^name left-right\n",
    "               ^location {<> start})\n",
    "--> \n",
    "    (halt)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-replication",
   "metadata": {},
   "source": [
    "Soar needs a couple of rl-specific rules for reinforcement learning to work.\n",
    "\n",
    "Take a second to guess what is going on. These will be explained in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "temporal-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rl_rules = \"\"\"\n",
    "sp {left-right*rl*left\n",
    "    (state <s> ^name left-right\n",
    "               ^operator <op> +) \n",
    "    (<op> ^name move\n",
    "          ^dir left)\n",
    "-->\n",
    "    (<s> ^operator <op> = 0)\n",
    "}\n",
    "\n",
    "sp {left-right*rl*right\n",
    "    (state <s> ^name left-right\n",
    "               ^operator <op> +) \n",
    "    (<op> ^name move\n",
    "          ^dir right)\n",
    "-->\n",
    "    (<s> ^operator <op> = 0)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "lr_reward = \"\"\"\n",
    "sp {elaborate*reward\n",
    "    (state <s> ^name        left-right\n",
    "               ^reward-link <r> \n",
    "               ^location    <d-name> \n",
    "               ^direction   <dir>)\n",
    "    (<dir> ^name   <d-name> \n",
    "           ^reward <d-reward>) \n",
    "-->\n",
    "    (<r> ^reward.value <d-reward>) \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-judgment",
   "metadata": {},
   "source": [
    "By default, RL is disabled. We must enable it by executing `rl --set learning on`. \n",
    "\n",
    "Let's run the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "useful-johns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- SOURCING PRODUCTIONS ------------\n",
      "Total: 8 productions sourced.\n",
      "rl --set learning on\n",
      "\n",
      "indifferent-selection -g\n",
      "\n",
      "step\n",
      "--> 1 decision cycle executed. 1 rule fired.\n",
      "print --rl\n",
      "left-right*rl*right  0.000000 0\n",
      "left-right*rl*left  0.000000 0\n"
     ]
    }
   ],
   "source": [
    "agent_raw = f\"\"\"\n",
    "{lr_initialization}\n",
    "{lr_operators}\n",
    "{lr_halt_condition}\n",
    "{lr_rl_rules}\n",
    "{lr_reward}\n",
    "\"\"\"\n",
    "\n",
    "lr_agent = SoarAgent(agent_raw=agent_raw)\n",
    "lr_agent.add_connector('lr-agent', AgentConnector(lr_agent))\n",
    "lr_agent.connect()\n",
    "\n",
    "# Enable RL in Soar\n",
    "lr_agent.execute_command('rl --set learning on', print_res=True)\n",
    "\n",
    "# Set exploration policy (explained later)\n",
    "# TODO This returns \"Too many args\", so using the shorthand `-g` instead\n",
    "# lr_agent.execute_command('indifferent-selection --epsilon-greedy', print_res=True)\n",
    "lr_agent.execute_command('indifferent-selection -g', print_res=True)\n",
    "\n",
    "# Step once\n",
    "lr_agent.execute_command('step', print_res=True);\n",
    "# TODO Expected output does not show up\n",
    "lr_agent.execute_command('print --rl', print_res=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-violin",
   "metadata": {},
   "source": [
    "You can read the output as saying, \"there is an indifferent probability of selecting either direction.\"\n",
    "\n",
    "`left-right*rl*left 0. 0` means that after 0 updates, we have a value of 0. \n",
    "\n",
    "This is expected as the Soar agent hasn't received any rewards.\n",
    "\n",
    "If we run the agent, it will recieve a reward and update the preference value for left/right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "extra-nylon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print --rl\n",
      "left-right*rl*right  1.000000 0.300000\n",
      "left-right*rl*left  0.000000 0\n"
     ]
    }
   ],
   "source": [
    "def learn(n):\n",
    "    for i in range(n):\n",
    "        # Reinitialize halted agent\n",
    "        lr_agent.execute_command('init')\n",
    "        # initialize-left-right\n",
    "        lr_agent.execute_command('step')\n",
    "        # Move operator\n",
    "        lr_agent.execute_command('step')\n",
    "        # Get agent to halt\n",
    "        lr_agent.execute_command('step')\n",
    "    \n",
    "learn(1)\n",
    "lr_agent.execute_command('print --rl', print_res=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-municipality",
   "metadata": {},
   "source": [
    "Even though we reinitialized Soar, the preference values don't get wiped out.  \n",
    "If they were cleared, our agent couldn't learn as easily.\n",
    "\n",
    "Let's see what happens when we run the agent 20 more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "tested-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print --rl\n",
      "left-right*rl*right  11.000000 0.980227\n",
      "left-right*rl*left  0.000000 0\n",
      "print --rl\n",
      "left-right*rl*right  19.000000 0.998860\n",
      "left-right*rl*left  2.000000 -0.510000\n"
     ]
    }
   ],
   "source": [
    "learn(10)\n",
    "lr_agent.execute_command('print --rl', print_res=True);\n",
    "learn(10)\n",
    "lr_agent.execute_command('print --rl', print_res=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-anaheim",
   "metadata": {},
   "source": [
    "If you had an output like mine, you may have noticed that even though our agent was pretty confident that `right` was the correct choice, it still tried moving `left` sometimes.\n",
    "\n",
    "To understand why the agent did this, we have to cover **exploration**.\n",
    "\n",
    "## Exploration\n",
    "\n",
    "Now let's cover the *exploration policy* we set.\n",
    "\n",
    "Take a look at this output.\n",
    "\n",
    "```\n",
    "run\n",
    "Moved: right\n",
    "This Agent halted.\n",
    "An agent halted during the run.\n",
    "\n",
    "init-soar\n",
    "Agent reinitialized.\n",
    "\n",
    "run\n",
    "Moved: right\n",
    "This Agent halted.\n",
    "An agent halted during the run.\n",
    "\n",
    "init-soar\n",
    "Agent reinitialized.\n",
    "\n",
    "run\n",
    "Moved: left\n",
    "This Agent halted.\n",
    "An agent halted during the run.\n",
    "```\n",
    "\n",
    "The agent moves twice to the right and therefore gets rewarded twice.\n",
    "Why would the agent then go left? \n",
    "Surely it has learned by now to prefer moving to the right.\n",
    "\n",
    "If you are familiar with optimization techniques like hill-climbing / stochastic gradient descent,\n",
    "you are probably aware that taking actions that lead to a lesser performance score\n",
    "may help in finding a new path. (TODO:REWORD)\n",
    "Otherwise known as escaping a local maximum to reach to global maximum by first traveling down.\n",
    "\n",
    "What if, for example, `right` was rewarded every time this program ran except for \n",
    "in the first fifteen minutes of every hour. \n",
    "During that time, `left` is rewarded.\n",
    "(For this example, on the state object, we are also taking into account `^hour-quarter` which is 1-4).\n",
    "\n",
    "If we started the agent at 3:16pm, it would learn to pick `right` after only a few moves.\n",
    "If it was stubborn and did not allow for future exploration of other possibilities,\n",
    "it would always choose `right` despite it being 4:06pm.\n",
    "\n",
    "By being explorative, the agent can choose to make the seemingly wrong choice in order to learn something new.\n",
    "\n",
    "\n",
    "## RL Rules\n",
    "\n",
    "This all begs the question, how does Soar map state-action pairs to a reward?\n",
    "\n",
    "This is done using *rl-rules*. \n",
    "We created two above but didn't explain the motivation.\n",
    "\n",
    "Soar maintains a number (referred to as the *q-value* in RL terminology) \n",
    "that denotes the expected value of an operator for a given state.\n",
    "\n",
    "Let's say that we added the `^hour-quarter` to the `move` operator and modified the above rl-rules to match on it.\n",
    "Then Soar would map the `^dir` and `^hour-quarter` to the operator's preference value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-draft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
